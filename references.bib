@misc{simonyan15,
    title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
    author={Karen Simonyan and Andrew Zisserman},
    year={2015},
    eprint={1409.1556},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{he15,
    title={Deep Residual Learning for Image Recognition}, 
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1512.03385},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@article{sarker21,
    author      = {Sarker, I. H.},
    title       = {Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},
    journal     = {SN Computer Science},
    volume      = {2},
    number      = {420},
    year        = {2021},
    url         = {https://doi.org/10.1007/s42979-021-00815-1}
}
@article{taye23,
    author      = {Taye, M. M.},
    title       = {Understanding of Machine Learning with Deep Learning: Architectures, Workflow, Applications and Future Directions},
    journal     = {Computers},
    volume      = {12},
    number      = {5},
    year        = {2023},
    pages       = {91},
    url         = {https://doi.org/10.3390/computers12050091}
}
@misc{hayou21,
    author      = {Soufiane Hayou and Jean-Francois Ton and Arnaud Doucet and Yee Whye Teh},
    title       = {Robust pruning at initialization},
    year        = {2021},
    eprint      ={2002.08797},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url         = {https://arxiv.org/abs/2002.08797}
}
@article{riera22,
    title       = {DNN pruning with principal component analysis and connection importance estimation},
    journal     = {Journal of Systems Architecture},
    volume      = {122},
    pages       = {102336},
    year        = {2022},
    issn        = {1383-7621},
    doi         = {https://doi.org/10.1016/j.sysarc.2021.102336},
    url         ={https://www.sciencedirect.com/science/article/pii/S1383762121002307},
    author      = {Marc Riera and José María Arnau and Antonio González},
}
@inproceedings{han15,
    author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Learning both Weights and Connections for Efficient Neural Network},
    url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
    volume = {28},
    year = {2015},
}
@inproceedings{lecun89,
    author = {LeCun, Yann and Denker, John and Solla, Sara},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {D. Touretzky},
    pages = {},
    publisher = {Morgan-Kaufmann},
    title = {Optimal Brain Damage},
    url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
    volume = {2},
    year = {1989}
}
@inproceedings{hassibi92,
    author = {Hassibi, Babak and Stork, David},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {S. Hanson and J. Cowan and C. Giles},
    pages = {},
    publisher = {Morgan-Kaufmann},
    title = {Second order derivatives for network pruning: Optimal Brain Surgeon},
    url = {https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},
    volume = {5},
    year = {1992}
}
@inproceedings{chauvin88,
    author = {Chauvin, Yves},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {D. Touretzky},
    publisher = {Morgan-Kaufmann},
    title = {A Back-Propagation Algorithm with Optimal Use of Hidden Units},
    url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf},
    volume = {1},
    year = {1988}
}
@misc{louizos18,
    title={Learning Sparse Neural Networks through $L_0$ Regularization}, 
    author={Christos Louizos and Max Welling and Diederik P. Kingma},
    year={2018},
    eprint={1712.01312},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@misc{molchanov17,
    title={Pruning Convolutional Neural Networks for Resource Efficient Inference}, 
    author={Pavlo Molchanov and Stephen Tyree and Tero Karras and Timo Aila and Jan Kautz},
    year={2017},
    eprint={1611.06440},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{janowski89,
    title = {Pruning versus clipping in neural networks},
    author = {Janowsky, Steven A.},
    journal = {Phys. Rev. A},
    volume = {39},
    issue = {12},
    pages = {6600--6603},
    numpages = {0},
    year = {1989},
    month = {6},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevA.39.6600},
    url = {https://link.aps.org/doi/10.1103/PhysRevA.39.6600}
}
@misc{han16,
    title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}, 
    author={Song Han and Huizi Mao and William J. Dally},
    year={2016},
    eprint={1510.00149},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{arora18,
    title={Stronger generalization bounds for deep nets via a compression approach}, 
    author={Sanjeev Arora and Rong Ge and Behnam Neyshabur and Yi Zhang},
    year={2018},
    eprint={1802.05296},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{mitsuno20,
    title={Filter Pruning using Hierarchical Group Sparse Regularization for Deep Convolutional Neural Networks}, 
    author={Kakeru Mitsuno and Takio Kurita},
    year={2020},
    eprint={2011.02389},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{he2017,
    title={Channel Pruning for Accelerating Very Deep Neural Networks}, 
    author={Yihui He and Xiangyu Zhang and Jian Sun},
    year={2017},
    eprint={1707.06168},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{frankle19,
    title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
    author={Jonathan Frankle and Michael Carbin},
    year={2019},
    eprint={1803.03635},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{snip19,
    title={SNIP: Single-shot Network Pruning based on Connection Sensitivity}, 
    author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip H. S. Torr},
    year={2019},
    eprint={1810.02340},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{grasp20,
    title={Picking Winning Tickets Before Training by Preserving Gradient Flow}, 
    author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
    year={2020},
    eprint={2002.07376},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{synflow20,
    title={Pruning neural networks without any data by iteratively conserving synaptic flow}, 
    author={Hidenori Tanaka and Daniel Kunin and Daniel L. K. Yamins and Surya Ganguli},
    year={2020},
    eprint={2006.05467},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{frankle21,
    title={Pruning Neural Networks at Initialization: Why are We Missing the Mark?}, 
    author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
    year={2021},
    eprint={2009.08576},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{su20,
      title={Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot}, 
      author={Jingtong Su and Yihang Chen and Tianle Cai and Tianhao Wu and Ruiqi Gao and Liwei Wang and Jason D. Lee},
      year={2020},
      eprint={2009.11094},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{singh21,
      title={Why is Pruning at Initialization Immune to Reinitializing and Shuffling?}, 
      author={Sahib Singh and Rosanne Liu},
      year={2021},
      eprint={2107.01808},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{liu22,
    title={The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training}, 
    author={Shiwei Liu and Tianlong Chen and Xiaohan Chen and Li Shen and Decebal Constantin Mocanu and Zhangyang Wang and Mykola Pechenizkiy},
    year={2022},
    eprint={2202.02643},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{evci21,
    title={Rigging the Lottery: Making All Tickets Winners}, 
    author={Utku Evci and Trevor Gale and Jacob Menick and Pablo Samuel Castro and Erich Elsen},
    year={2021},
    eprint={1911.11134},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{pham23,
    title={Understanding Pruning at Initialization: An Effective Node-Path Balancing Perspective},
    author={Hoang Pham and Anh Ta and Shiwei Liu and Dung D. Le and Long Tran-Thanh},
    year={2023},
    url={https://openreview.net/forum?id=nqoxB03tzi}
}
@misc{gebhart21,
    title={A Unified Paths Perspective for Pruning at Initialization}, 
    author={Thomas Gebhart and Udit Saxena and Paul Schrater},
    year={2021},
    eprint={2101.10552},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{jacot20,
    title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks}, 
    author={Arthur Jacot and Franck Gabriel and Clément Hongler},
    year={2020},
    eprint={1806.07572},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{neyshabur15,
    title={Norm-Based Capacity Control in Neural Networks}, 
    author={Behnam Neyshabur and Ryota Tomioka and Nathan Srebro},
    year={2015},
    eprint={1503.00036},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}