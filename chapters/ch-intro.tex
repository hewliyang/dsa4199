\SetPicSubDir{ch-Intro}

\chapter{Introduction}
\vspace{1em}

\section{Terminology}
Some terminologies that are used throughout this paper are shown below.
\begin{itemize}
    \item An \textit{overparameterized} neural network is a type of neural network model that has a significantly larger number of parameters (weights and biases) than the number that is theoretically necessary.
    \item In the context of neural network pruning, \textit{heuristics} refer to strategies or criteria that guide the selection of which network components (weights, neurons, filters) to prune, typically based on empirical observations.
    \item \textit{Fine-tuning} refers to the process of retraining a pruned neural network to recover or improve its performance.
    \item \textit{Sparsity} (denoted by $\kappa$ \footnote{The common notation for sparsity in most literature is $s$, but $\kappa$ is used to avoid confusion with scores $s_i$.}), more specifically, parameter sparsity, refers to the fraction of weights in the network that we wish to prune (i.e., zero-valued weights).\\
    (FLOP sparsity, which is the number of arithmetic operations that can be skipped due to zero-valued inputs or weights, will not be discussed.)
    \item \textit{Compression ratio} (denoted by $\rho$) is defined as the original number of parameters divided by the number of remaining parameters remaining in the pruned network, i.e., $\rho = \frac{1}{1-\kappa}$.
    \item \textit{Saliency} is the measure of importance of individual network component based on their contribution to the overall performance of the network.
    \item \textit{Sensitivity} refers to how responsive the network's output is to changes in the input or weights in the network. Sensitivity-based pruning evaluates the importance of a weight by $\Big| \textbf{w} \dfrac{\partial \mathcal{L}}{\partial \textbf{w}}\Big|$, where $\textbf{w}$ is a weight and $\mathcal{L}$ is the loss, as a measure of sensitivity \autocite{hayou21}.
    \item \textit{Gradient flow}, in the context of neural networks, refers to the backward propagation of gradients (derivatives of the loss function) during training.
    \item \textit{Pruning mask} is a binary mask that is applied to the parameters of a neural network to identify parameters to be retained (assigned a value of 1) and parameters to be pruned (assigned a value of 0).
    \item \textit{VGG}, or Visual Geometry Group, is a deep Convolutional Neural Network (CNN) architecture, introduced in \textcite{simonyan15}. The number (in VGG-16, VGG-19, etc.) refers to the number of layers in the architecture.
    \item \textit{ResNet}, or Residual Network, is another deep neural network architecture, introduced in \textcite{he15}. These networks include \textit{skip connections}, which allow neurons from non-adjacent layers to be connected, with the aim to solve the problem of vanishing gradient in deep networks.
    \item In this paper, a \textit{path} is defined as a sequence of connected neurons, starting from an input neuron to an output neuron. An \textit{effective path} is a path where all connections are non-zero. An \textit{effective neuron} is a neuron that belongs to at least one effective path.

\end{itemize}
\section{Motivation and Background}
Within the dynamic domain of deep learning, neural networks have emerged as a driving force behind significant progress in artificial intelligence. These networks form the foundation for a wide range of applications, including image recognition, natural language processing, autonomous systems, cybersecurity and medical diagnostics \autocite{sarker21}. The success of deep neural networks can be primarily attributed to their capacity to model intricate patterns within large volumes data \autocite{taye23}. Overparameterized deep neural networks have been shown to achieve state of the art (SOTA) performance in a variety of tasks; however, they demands substantial computational resources and memory, making it impossible to implement these models on devices with limited computational resources \autocite{hayou21}. It is thus essential to find ways to enhance model efficiency while maintaining its accuracy.

The concept of neural network pruning has become popular as a crucial technique in the quest for enhancing efficiency without compromising on model performance. The underlying idea behind neural network pruning is based on the observation that not all components within a deep neural network make the same contribution to the overall performance. Overparameterized neural networks contain redundant or unimportant weights and/or neurons. Therefore, the key motivation behind pruning techniques is to remove such components, resulting in leaner models without sacrificing prediction accuracy \autocite{riera22}. This not only reduces the computational load but also increases model interpretability, ease of deployment, energy efficiency \autocite{han16}, and better generalization bounds \autocite{arora18}. These attributes make pruning an essential tool for implementing deep neural networks in resource-constrained environments.

To decide which weights or neurons can be considered as unimportant, saliency-based pruning techniques employ a variety of heuristics to selectively remove these weights or neurons. One example is magnitude-based pruning, where weights or neurons with small absolute values are considered less important and are thus pruned. This leverages on the observation that small weights have a limited impact on the whole network and can be dropped with minimal loss in performance. \textcite{han15} introduces fine-tuning with $L_2$ regularization and iteratively removes connections which have values below a chosen threshold. Other similar work using magnitude-based pruning includes \textcite{janowski89}, \textcite{han16}.

Some other popular scoring metrics are based on the derivative of the loss function with respect to the weight being considered. Optimal Brain Damage \autocite{lecun89} and Optimal Brain Surgeon \autocite{hassibi92} propose criteria based on the Hessian of the loss function with a second-order Taylor expansion of an objective function. A more recent work, \textcite{molchanov17} introduce scoring based on a first-order Taylor expansion to estimate the change in the cost function resulting from the removal of parameters.

A different approach to enforce sparsity is to introduce a sparsity penalty term. For example, \textcite{chauvin88} and more recently, \textcite{louizos18} include a penalty term ($L_0$ or $L_1$ regularization). For deep convolutional neural networks, group sparse regularization can be used as a basis for structured weight pruning to remove less important filters or clusters. Group sparse regularization aims to introduce sparsity by grouping related parameters together and applying sparsity constraints to these groups. \textcite{mitsuno20} introduces the concept of hierarchical group sparse regularization, which promotes the network to maintain a structured sparsity pattern where entire groups of filters are either kept or removed together. \textcite{he2017} obtains sparse networks at group levels by channel pruning based on LASSO regression with iterative fine-tuning to preserve accuracy.

While these pruning techniques have achieved excellent results compared to the unpruned neural networks, the cost of training remains the same as most pruning techniques can only be implemented after having trained the full neural network \autocite{hayou21}. The Lottery Ticket Hypothesis introduced in \textcite{frankle19} shows the existence of sparse subnetworks (``winning tickets'') from early in training that can be trained in isolation to competitive performance to the original neural network. This finding shows the possibility of obtaining a model from pruning at initialization with similar performance as those obtained from pruning after training.

Similar to pruning after training, pruning at initialization is also often based on a specific criterion. SNIP \autocite{snip19} introduces scoring based on connection sensitivity of each weight, which is its effect on the loss, and prunes the weights with the least substantial (positive or negative) effect. GraSP \autocite{grasp20} aims to preserve gradient flow, as a pruned network will have fewer neurons and connections, affecting the flow of gradients. GraSP finds and removes weights with the least benefit to the gradient flow using a Hessian based approach. SynFlow \autocite{synflow20} shows that existing gradient-based pruning procedures at initialization face the problem of layer collapse, where an entire layer is pruned, making the resulting network untrainable. To avoid layer collapse, SynFlow aims to preserve the total flow of synaptic strengths through the network by iteratively prunes weights with the lowest synaptic strengths without taking the data into account. \textcite{hayou21} presents an algorithm for sensitivity-based pruning with principled scaling and re-parameterization. These techniques are able to achieve state-of-the-arts results on datasets such as MNIST, CIFAR10, CIFAR100, TinyImageNet and ImageNet.

However, as shown in \textcite{frankle21}, SNIP, GraSP, and SynFlow are still unable to achieve the same performance as magnitude pruning after training. The authors discovered that randomly shuffling the weights pruned by these methods within each layer and reinitializing the weights have insignificant impact on the accuracy of the model, and thus questioned the choice of pruning criteria and pruning at initialization as a whole.

\textcite{frankle21} includes a very interesting observation that the accuracy of SynFlow improves at extreme sparsities after randomly shuffling layer-wise. The authors attributed this behavior to a phenomenon termed ``neuron collapse'', which is when entire neurons are pruned (i.e., all the connection weights to a neuron are pruned), and highlighted that SynFlow prunes entire neurons more often than other methods.

\textcite{synflow20} does address the findings of \textcite{frankle21} and asserts that the ablation studies do not challenge the premise behind SynFlow, which is to prevent layer collapse. The authors highlight the significant drop in the performance of SynFlow after inverting the scores (i.e., the most important weights are pruned first), as compared to SNIP and GraSP, as proof that SynFlow's algorithm successfully achieves its purpose. However, \textcite{synflow20} does not address the improved performance of SynFlow after layerwise shuffling at extreme sparsities. 

In this paper, we investigate the accuracy of SynFlow-pruned networks before and after random layerwise shuffling to verify if neuron collapse is indeed the reason behind this behavior. We seek to elucidate the following key questions:
\begin{itemize}
    \item Is neuron collapse more likely to happen for SynFlow?
    \item Is neuron collapse the cause of SynFlow's improved performance after shuffling?
\end{itemize}
The code for SNIP, GraSP, and SynFlow used to obtain the results in this paper is largely based on the one used in \textcite{synflow20} \footnote{The code can be found at \url{https://github.com/ganguli-lab/Synaptic-Flow}.} \footnote{Unfortunately, \textcite{frankle21} have not released their open-source implementations and experiments used in the paper, so we tried to replicate some experiments with the implementations from \textcite{synflow20}.}, with some additions for our experiments.

% The rest of this thesis is organized as follows. 
% In \autoref{ch:review}, we conduct a more in-depth literature review. 
% \autoref{ch:rice} provides the study on rice. 
% \autoref{ch:noodle} describes noodles. 
% We conclude the entire thesis as well as discuss further directions for future research in \autoref{ch:concl}.
