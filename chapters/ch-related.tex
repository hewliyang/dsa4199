\SetPicSubDir{ch-Related}
\SetExpSubDir{ch-Related}

\chapter{Related work}
\label{ch:related}
\vspace{1em}

\textcite{su20} also conducted a series of ablations as ``sanity checks'' for the premise behind techniques for pruning at initialization. The pruning techniques used include SNIP and GraSP. Experiments, including layerwise shuffling, were done to evaluate whether the architecture of the pruned subnetwork is important to the final performance. The results were consistent with those from \textcite{frankle21}: the performance did not decrease after layerwise shuffling when tested with SNIP and GraSP.

Ablations on the data (introducing random labels, shuffling the pixels) were also done to assess whether these techniques leverage information from the training data during the pruning phase to identify effective subnetworks. Interestingly, the performance was also unaffected by these ablations, highlighting the weak dependency on the data of these methods.

The work of \textcite{su20} also confirmed the hypothesis by the authors of \textcite{frankle21} that  the techniques for pruning at initialization identify layerwise sparsity ratio, not the individual weights for pruning. ``Keep-ratios'' were extracted from SNIP and GraSP and random pruning based on these ratios achieved similar performance. The authors observed that the ratios drop with depth and tested with derived layerwise keep-ratios using simple decreasing functions. The final performance was also similar to those from SNIP and GraSP.

Similarly, \textcite{liu22} also obtained layerwise sparsity ratios from SNIP and GraSP and performed random pruning with these ratios. The results show that the resulting pruned subnetwork of Wide ResNet-50 can be trained to outperform the dense, unpruned Wide ResNet-50 network. It is also found that random pruning with sparsity ratios based on Erd√µs-Renyi-Kernel (ERK) \autocite{evci21}, where larger layers are assigned higher sparsities, outperforms pruning with SNIP and GraSP.

A follow-up study by \textcite{singh21} seeks to elucidate the insensitivity of pruned networks to ablations observed in the works of \textcite{frankle21} and \textcite{su20}. By measuring the average Wasserstein distance between the weight distribution of the original pruned network and that of the pruned network after each ablation (reinitialization, layerwise shuffling), the authors found that compared to random pruning, the weight distributions after reinitialization and layerwise shuffling are significantly more similar to the distribution of the original pruned network. As such, \textcite{singh21} hypothesizes that as the weight distributions are largely preserved, the performances after these ablations are similar to the performance of the original pruned network. However, as acknowledged by the authors, this does not offer the full explanation for the insensitivity to ablations.

A recent work by \textcite{pham23} \footnote[6]{This paper is currently still under open review.} offers an alternative explanation through the introduction of a concept termed ``node-path balancing principle''. The authors hypothesized that layerwise shuffling leads to an increase in the number of effective neurons and a decrease in the number of effective paths in SynFlow-pruned networks. This in turn results in an improvement in the representation capacity of subnetworks with more effective nodes, while the information flow remains unaffected with a sufficient number of effective paths. The authors also proposed a new pruning scheduler based on SynFlow which replaces the pruning heuristics with random pruning in some early iterations with the aim of achieving a balance between effective nodes and paths. 

\textcite{gebhart21} also considers the path structure of the network and proposes the Path Kernel, based on the Neural Tangent Kernel \autocite{jacot20}. A variant of SynFlow, called SynFlow-L2, is introduced, which is shown to outperform SynFlow in wider networks. 
