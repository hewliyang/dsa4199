\begin{abstract}

Pruning techniques at initialization have gained prominence for their ability to reduce the size of neural networks while preserving performance. However, a puzzling phenomenon observed in such techniques is their insensitivity to layerwise shuffling, as highlighted in the seminal work of \textcite{frankle21}. Particularly, the performance of SynFlow-pruned networks at extreme sparsities improved after shuffling. Our investigation not only reaffirms the presence of neuron collapse, originally proposed by \textcite{frankle21} as the cause of this improved performance, but also uncovers its occurrence in SNIP and GraSP. Additionally, we introduce novel scoring adjustments, such as SynFlow-Square, SynFlow-Cube, and SynFlow-Exp, which significantly increase the percentage of effective neurons. While these adjustments lead to sensitivity to layerwise shuffling at 99\% sparsity in certain networks, the same effect does not manifest at 90\% sparsity, which shows that neuron collapse does not fully explain the improved performance post-shuffling. To further investigate this puzzle, we conduct a case study involving magnitude-based pruning. Contrary to expectations, shuffling magnitude-pruned networks results in increased accuracy, despite these networks having more effective paths and neurons before shuffling. These findings underscore the complexity of the relationship between neuron collapse, the number of effective paths and neurons, and sensitivity to layerwise shuffling.

% In conclusion, our study sheds light on the intricate behavior of pruned neural networks, highlighting the presence of neuron collapse and its nuances. While neuron collapse and the number of effective paths and neurons offer valuable insights, they do not provide a complete explanation for the observed phenomena. Our research calls for further exploration to unravel the mechanisms governing pruned networks at varying sparsity levels and to optimize their behavior.

\end{abstract}
